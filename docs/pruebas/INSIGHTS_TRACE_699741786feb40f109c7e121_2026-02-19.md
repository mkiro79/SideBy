# Traza completa del flujo Insights (Docker)

## 1) Objetivo de la prueba
Validar, con ejecución real en contenedor Docker, qué tarda en el endpoint de insights y qué variables entran/salen por cada etapa.

- Dataset: `699741786feb40f109c7e121`
- Usuario: token JWT (usado en ejecución, **no expuesto completo en este reporte**)
- Endpoint: `GET /api/v1/datasets/:id/insights`
- Filtros usados: `{ "categorical": {} }`

---

## 2) Evidencia generada

- Traza JSON completa (entrada/salida):
  - `docs/pruebas/insights-flow-raw.json`
- Logs API (últimos 15 min):
  - `docs/pruebas/insights-flow-api-logs-raw.txt`
- Script de trazabilidad ejecutado en contenedor:
  - `docs/pruebas/trace-insights-flow.ts`

---

## 3) Secuencia de ejecución (paso a paso)

### Paso A - Request 1 (forzar regeneración)
**Input**
- URL: `http://localhost:3000/api/v1/datasets/699741786feb40f109c7e121/insights?filters=%7B%22categorical%22%3A%7B%7D%7D&forceRefresh=true`
- Método: `GET`
- Auth: `Bearer <token>`

**Output**
- HTTP `200`
- `meta.cacheStatus`: `miss`
- `meta.generationSource`: `rule-engine`
- `meta.narrativeStatus`: `generated`
- `meta.total`: `26`
- `insightsCount`: `26`
- `businessNarrative`: `true`

**Tiempo**
- `elapsedMs`: **15465 ms**
- `meta.generationTimeMs`: **15428 ms**

---

### Paso B - Request 2 (sin forzar, con caché)
**Input**
- URL: `http://localhost:3000/api/v1/datasets/699741786feb40f109c7e121/insights?filters=%7B%22categorical%22%3A%7B%7D%7D`
- Método: `GET`
- Auth: `Bearer <token>`

**Output**
- HTTP `200`
- `meta.cacheStatus`: `hit`
- `meta.generationSource`: `rule-engine`
- `meta.narrativeStatus`: `generated`
- `meta.total`: `26`
- `insightsCount`: `26`
- `businessNarrative`: `true`

**Tiempo**
- `elapsedMs`: **105 ms**
- `meta.generationTimeMs`: **100 ms**

---

### Paso C - Snapshot de dataset leído en flujo
**Input real al motor**
- `status`: `ready`
- `totalRows`: `19331`
- `rowsByGroup`: `groupA=10228`, `groupB=9103`
- `kpiFields`: `total_sends`, `total_opens`, `total_clicks`, `total_applies`
- `categoricalFields`: `country`, `source`, `source_detail`, `page`
- `aiConfig.enabled`: `true`

**KPIs agregados por grupo (calculados por RuleEngine)**
- `total_sends`: groupA `3405060465`, groupB `3777803613` (≈ +10.95%)
- `total_opens`: groupA `445590779`, groupB `480336195` (≈ +7.80%)
- `total_clicks`: groupA `26985749`, groupB `30148027` (≈ +11.72%)
- `total_applies`: groupA `15051523`, groupB `11446789` (≈ -23.95%)
- `overallChange` promedio: **+1.6283%**

---

### Paso D - Tiempos internos RuleEngine (medidos)
- `applyFilters`: **0 ms**
- `calculateKPIs`: **16 ms**
- `detectDimensionalOutliers`: **46 ms**
- `findTopPerformer`: **2 ms**
- `calculateOverallChange`: **0 ms**
- `generateInsightsTotal`: **46 ms**

**Output RuleEngine**
- `insightsCount`: `26`
- Distribución:
  - `anomaly`: `24`
  - `suggestion`: `1`
  - `summary`: `1`
- `outliersCount`: `24`

---

## 4) Diagnóstico técnico de por qué tarda ~10-15s

Con la evidencia de arriba:

- El `RuleEngine` tarda **~46 ms**.
- El endpoint en `forceRefresh=true` tarda **~15.4 s**.
- Diferencia aproximada atribuible a narrativa LLM + overhead: `15428 - 46 = ~15382 ms`.

**Conclusión:** el cuello de botella principal es la generación de `businessNarrative` (LLM), no el cálculo de reglas.

---

## 5) Diagnóstico de por qué “no cuadra” el contraste entre grupos

Según implementación actual de `RuleEngineAdapter`:

1. **El contraste A vs B solo se usa para KPI globales** (`calculateKPIs` y `overallChange`).
2. **Las anomalías se calculan sobre el dataset combinado** (no separa outlier por grupo A vs grupo B), usando z-score por dimensión/kpi en totales.
3. Por eso aparecen muchas anomalías de magnitud alta en dimensiones con distribución sesgada, aunque el cambio promedio A vs B sea moderado.

**Efecto práctico observado:**
- No salen insights de tipo `trend/warning` por regla de `>30%` global, porque los KPIs globales no cruzan ese umbral.
- Sí salen muchos `anomaly` por outliers dimensionales en totales combinados.

---

## 6) Resumen ejecutivo

- El tiempo largo no lo causa el JSON ni el RuleEngine; lo causa la narrativa LLM en cold/generate (`~15s`).
- El modelo actual de anomalías está orientado a outliers por dimensión total, no a comparación estricta `groupA vs groupB` por dimensión.
- Si el objetivo funcional es “contrastar grupos”, hay que ajustar la lógica de anomalías para calcular deltas por dimensión entre A y B (en vez de z-score sobre mezcla).
